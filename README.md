
# Model Fusion Alpha Optimization

## Overview

This repository is an extension of the original [Pruning-via-Merging](https://github.com/SempraETY/Pruning-via-Merging) implementation for model fusion. The original codebase provides the core framework for fusing model layers and evaluating them.

Our contribution is an automated optimization pipeline to **find the optimal layer fusion coefficients ($\alpha$ values)**. This allows us to move beyond manual tuning and systematically discover alpha sets that maximize performance, either for general accuracy or for specific, targeted subjects (e.g., MMLU subjects like `anatomy` or `abstract_algebra`).

## Our Additions

We have introduced several new scripts and a methodology for comparing different alpha-generation techniques.

  * **`optimize_alphas.py`**: Uses **Bayesian Optimization** to find the best alpha values for a given objective.
  * **`alpha_pipeline.py`**: Runs the full fusion and MMLU evaluation pipeline using a specified alpha file.
  * **`parthiv-alphas.json`**: Alpha values generated using a **Gradient Descent**-based approach.
  * **`fusion_alphas.json`**: Baseline values of alphas (merged using 13 layers no alpha optimization)
  * **`optimized_alphas.json`**: `optimize_alphas.py` results

## Optimization Methodologies

We compare results from two different optimization methods against a manual baseline.

### 1\. Gradient Descent

The results in the `./parthiv` directory are generated from the alphas in `parthiv-alphas.json`. This set of coefficients was determined using a gradient-descent-based optimization method.

### 2\. Bayesian Optimization

The `optimize_alphas.py` script treats the entire fusion-and-evaluation process as a "black-box" function.

1.  **Objective Function**: We define an objective to maximize (e.g., `average_accuracy` or a specific subject like `anatomy`).
2.  **Black Box**: The script repeatedly:
      * Proposes a new set of alpha values.
      * Runs the fusion and MMLU evaluation.
      * Reads the resulting score (e.g., `anatomy` accuracy).
3.  **Optimization**: A Bayesian Optimization algorithm observes these (alpha set, score) pairs and intelligently decides the next best alpha set to try, balancing exploration (trying new, uncertain values) and exploitation (refining known good values).

This process allows us to create **specialized models**. By simply changing the target objective in `optimize_alphas.py` (e.g., from `anatomy` to `abstract_algebra`), we can re-run the optimization to generate a new `optimized_alphas.json` file tailored to that specific subject.

The logs from the optimization process itself (showing the search progression) are saved in the `/optimization_run` directory.

## Repository Structure (Our Files)

```
.
├── alpha_pipeline.py       # Runs the full pipeline (fusion + eval) for a given alpha file
├── optimize_alphas.py      # Runs Bayesian Optimization to find the best alphas
|
├── fusion_alphas.json      # Baseline/manual alpha values
├── optimized_alphas.json   # Alphas generated by optimize_alphas.py
├── parthiv-alphas.json     # Alphas generated by Parthiv (gradient descent)
|
├── base/                   # Results from running alpha_pipeline.py with fusion_alphas.json
├── optimized/              # Results from running alpha_pipeline.py with optimized_alphas.json
├── parthiv/                # Results from running alpha_pipeline.py with parthiv-alphas.json
|
└── optimization_run/       # Logs generated by the optimize_alphas.py script
```

  * **Original Files**: All other scripts (`calib_tools.py`, `categories.py`, `crop.py`, etc.) are part of the original implementation and are called by our pipeline scripts.

## How to Run

### Step 1: Run the Optimization (Optional)

To generate a new set of optimized alphas, configure your objective in `optimize_alphas.py` and run it:

```bash
python optimize_alphas.py
```

This will run the full optimization search and produce `optimized_alphas.json` and logs in `optimization_run/`.

> [!NOTE]
> The following steps may overwrite the contents of the `base/`, `parthiv/`, and `optimized/` directories. Ensure you have backed up any important data before running the pipeline.

### Step 2: Run the Full Pipeline

To fuse a model and evaluate it using any of the available alpha files, use `alpha_pipeline.py`.

```bash
# Run with baseline alphas
python alpha_pipeline.py --model_path "meta-llama/Meta-Llama-3-8B" --alphas_file "./fusion_alphas.json" --num_layer 13 --output_dir "./base" --data_dir "./data"

# Run with Bayesian Optimization alphas
python alpha_pipeline.py --model_path "meta-llama/Meta-Llama-3-8B" --alphas_file "./optimized_alphas.json" --num_layer 13 --output_dir "./optimized" --data_dir "./data"

# Run with Gradient Descent alphas
python alpha_pipeline.py --model_path "meta-llama/Meta-Llama-3-8B" --alphas_file "./parthiv_alphas.json" --num_layer 13 --output_dir "./parthiv" --data_dir "./data"
```

## Alphas and MMLU Summary

### Average MMLU Accuracy

| Model | Average Accuracy |
|---|---|
| base (fusion_alphas.json) | 0.64710 |
| optimized (optimized_alphas.json) | 0.64705 |
| parthiv (parthiv-alphas.json) | 0.64753 |
| optimized_all (optimized_all_alphas.json) | 0.64888 |

### Alpha values (ratio_i_alpha) per merge step

Each row corresponds to a layer merge of the form into<-remove.

| Step | Merge (into<-remove) | base | optimized | parthiv | optimized_all |
|---:|:---:|---:|---:|---:|---:|
| 0 | 30<-31 | 0.6257765467 | 0.4668088019 | 0.509012 | 0.3249944307 |
| 1 | 29<-30 | 0.6270365984 | 0.5881297974 | 0.594924 | 0.3254301426 |
| 2 | 28<-29 | 0.6218680744 | 0.3000457499 | 0.710577 | 0.3458614108 |
| 3 | 27<-28 | 0.6279836130 | 0.4209330291 | 0.557285 | 0.6730300922 |
| 4 | 26<-27 | 0.6236575608 | 0.3587023563 | 0.701547 | 0.3886671637 |
| 5 | 25<-26 | 0.6261533059 | 0.3369354379 | 0.540713 | 0.5532461803 |
| 6 | 24<-25 | 0.6230820117 | 0.3745040846 | 0.715259 | 0.6088977355 |
| 7 | 23<-24 | 0.6222846654 | 0.4382242908 | 0.516398 | 0.3626643352 |
| 8 | 22<-23 | 0.6240776121 | 0.4587069897 | 0.623941 | 0.3015083045 |
| 9 | 21<-22 | 0.6122476283 | 0.5155266936 | 0.534059 | 0.3400057862 |
| 10 | 20<-21 | 0.6201869909 | 0.4676778058 | 0.700754 | 0.3435758257 |
| 11 | 19<-20 | 0.6139920247 | 0.5740878002 | 0.524230 | 0.3209859709 |
| 12 | 18<-19 | 0.6216273821 | 0.3817808999 | 0.602134 | 0.6283643694 |
